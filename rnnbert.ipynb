{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "balanced-postcard",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Dropout\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_text as text\n",
    "from transformers import *\n",
    "\n",
    "np.set_printoptions(suppress=True)\n",
    "import shutil\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "\n",
    "PATH = '/Users/farahmasood/Desktop/scedev/'\n",
    "dataset = pd.read_csv(PATH+'labeled_data.csv')\n",
    "\n",
    "nRowsRead = None\n",
    "dataset0 = pd.read_csv('/Users/farahmasood/Desktop/scedev/labeled_data.csv', delimiter=',', nrows = nRowsRead)\n",
    "dataset0.name = 'labeled_data.csv'\n",
    "nRow, nCol = dataset0.shape\n",
    "\n",
    "c=dataset0['class']\n",
    "dataset0.rename(columns={'tweet' : 'text',\n",
    "                   'class' : 'category'}, \n",
    "                    inplace=True)\n",
    "a=dataset0['text']\n",
    "b=dataset0['category'].map({0: 'hate_speech', 1: 'offensive_language',2: 'neither'})\n",
    "\n",
    "dataset= pd.concat([a,b,c], axis=1)\n",
    "dataset.rename(columns={'class' : 'label'}, \n",
    "                    inplace=True)\n",
    "\n",
    "\n",
    "hate, ofensive, neither = np.bincount(dataset['label'])\n",
    "total = hate + ofensive + neither"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "noted-plane",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "categorical_labels = to_categorical(np.array(dataset['label']),num_classes=3)\n",
    "\n",
    "X_train_, X_test, y_train_, y_test = train_test_split(\n",
    "    dataset.index.values,\n",
    "    categorical_labels,\n",
    "    test_size=0.10,\n",
    "    random_state=42,\n",
    "    stratify=categorical_labels,    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ongoing-zimbabwe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " ...\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print(y_train_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "intensive-wagner",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_, X_vals, y_train, y_vals = train_test_split(\n",
    "    X_train_,\n",
    "    y_train_,\n",
    "    test_size=0.10,\n",
    "    random_state=42,\n",
    "    stratify= y_train_,  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "flush-leather",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['data_type'] = ['not_set']*dataset.shape[0]\n",
    "dataset.loc[X_train_, 'data_type'] = 'train'\n",
    "dataset.loc[X_vals, 'data_type'] = 'vals'\n",
    "dataset.loc[X_test, 'data_type'] = 'test'\n",
    "\n",
    "dataset.groupby(['category', 'label', 'data_type']).count()\n",
    "\n",
    "dataset_train = dataset.loc[dataset[\"data_type\"]==\"train\"]\n",
    "dataset_vals = dataset.loc[dataset[\"data_type\"]==\"vals\"]\n",
    "dataset_test = dataset.loc[dataset[\"data_type\"]==\"test\"]\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((dataset_train.text.values, dataset_train.label.values))\n",
    "vals_dataset = tf.data.Dataset.from_tensor_slices((dataset_vals.text.values, dataset_vals.label.values))\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((dataset_test.text.values, dataset_test.label.values))\n",
    "\n",
    "train_dataset = train_dataset.shuffle(len(dataset_train)).batch(32, drop_remainder=False)\n",
    "vals_dataset = vals_dataset.shuffle(len(dataset_vals)).batch(32, drop_remainder=False)\n",
    "test_dataset = test_dataset.shuffle(len(dataset_test)).batch(32, drop_remainder=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "taken-fifth",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Using /var/folders/sd/9h__nn_90fq1rv5jz40nz6180000gn/T/tfhub_modules to cache modules.\n",
      "INFO:absl:Downloading TF-Hub Module 'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/1'.\n",
      "INFO:absl:Downloaded https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/1, Total size: 3.22MB\n",
      "INFO:absl:Downloaded TF-Hub Module 'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/1'.\n",
      "INFO:absl:Downloading TF-Hub Module 'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1'.\n",
      "INFO:absl:Downloaded https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1, Total size: 115.55MB\n",
      "INFO:absl:Downloaded TF-Hub Module 'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1'.\n"
     ]
    }
   ],
   "source": [
    "tf_encoder = 'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1'\n",
    "preprocess = 'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/1'\n",
    "\n",
    "bert_pre = hub.KerasLayer(preprocess)\n",
    "\n",
    "for text_batch, label_batch in train_dataset.take(1):\n",
    "  for i in range(1):\n",
    "    tweet = text_batch.numpy()[i]\n",
    "    label = label_batch.numpy()[i]\n",
    "\n",
    "text_test = ['this is such an amazing movie!']\n",
    "text_test = [tweet]\n",
    "\n",
    "preprocessed = bert_pre(text_test)\n",
    "bert_model = hub.KerasLayer(tf_encoder)\n",
    "results = bert_model(preprocessed)\n",
    "\n",
    "weight0 = (1 / hate)*(total)/3.0 \n",
    "weight1 = (1 / ofensive)*(total)/3.0\n",
    "weight2 = (1 / neither)*(total)/3.0\n",
    "weights = {0: weight0, 1: weight1, 2: weight2}\n",
    "bias = np.array([3.938462, 15, 5.])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "missing-parish",
   "metadata": {},
   "outputs": [],
   "source": [
    "def RNNmodel():\n",
    "    text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text')\n",
    "    preprocessing_layer = hub.KerasLayer(preprocess, name='preprocessing')\n",
    "    encoder_inputs = preprocessing_layer(text_input)\n",
    "    encoder = hub.KerasLayer(tf_encoder, trainable=True, name='BERT_encoder')\n",
    "    outputs = encoder(encoder_inputs)\n",
    "    net = sequence_output = outputs[\"sequence_output\"] # [batch_size, seq_length, 768]\n",
    "    \n",
    "    net = tf.keras.layers.Dense(512, activation=\"relu\")(net)\n",
    "    net = tf.keras.layers.LSTM(32)(net)\n",
    "    net = tf.keras.layers.Dropout(0.2)(net)\n",
    "    net = tf.keras.layers.Dense(3, activation=\"softmax\", name='classifier')(net)\n",
    "    return tf.keras.Model(text_input, net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "surrounded-pharmacy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[0.5508838 0.6257106 0.5700116]], shape=(1, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "rnn = RNNmodel()\n",
    "bertresult = rnn(tf.constant(text_test))\n",
    "print(tf.sigmoid(bertresult))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "russian-language",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "statutory-method",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:using Adamw optimizer\n",
      "INFO:absl:gradient_clip_norm=1.000000\n"
     ]
    }
   ],
   "source": [
    "from official.nlp import optimization\n",
    "epochs = 80\n",
    "steps_per_epoch = tf.data.experimental.cardinality(train_dataset).numpy()\n",
    "num_train_steps = steps_per_epoch * epochs\n",
    "num_warmup_steps = int(0.1*num_train_steps)\n",
    "\n",
    "init_lr = 3e-5\n",
    "optimizer = optimization.create_optimizer(init_lr=init_lr,\n",
    "                                          num_train_steps=num_train_steps,\n",
    "                                          num_warmup_steps=num_warmup_steps,\n",
    "                                          optimizer_type='adamw')\n",
    "\n",
    "rnn.compile(optimizer=optimizer,\n",
    "                          loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                          metrics=tf.keras.metrics.SparseCategoricalAccuracy('accuracy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "needed-harbor",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78/78 [==============================] - 101s 1s/step - loss: 0.8034 - accuracy: 0.7760\n",
      "Loss: 0.8041883111000061\n",
      "Accuracy: 0.7732957005500793\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = rnn.evaluate(test_dataset)\n",
    "\n",
    "print(f'Loss: {loss}')\n",
    "print(f'Accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "entertaining-islam",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
