{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "impossible-serve",
   "metadata": {},
   "source": [
    "# SVM 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "otherwise-tragedy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "saved-venue",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import transformers as ppb\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "handmade-jurisdiction",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.RandomState(228)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "capable-religious",
   "metadata": {},
   "source": [
    "## load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "entertaining-diameter",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\n",
    "    'https://github.com/clairett/pytorch-sentiment-classification/raw/master/data/SST2/train.tsv',\n",
    "    delimiter='\\t',\n",
    "    header=None\n",
    ")\n",
    "df = df.rename(columns={0 : \"text\", 1: \"class\"})\n",
    "\n",
    "print(df.head())\n",
    "print(df.shape, end=\"\\n\\n\")\n",
    "print(df.iat[0, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "casual-testimony",
   "metadata": {},
   "source": [
    "### reduce corpus size for convenience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "billion-rebel",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[:2500]\n",
    "\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hidden-entity",
   "metadata": {},
   "source": [
    "## load pretrained BERT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fewer-singles",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_tokenizer = ppb.DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "bert_model = ppb.DistilBertModel.from_pretrained('distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stone-clock",
   "metadata": {},
   "source": [
    "## use BERT\n",
    "\n",
    "### BERT preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "identical-fundamentals",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bert_preprocess(text):\n",
    "    \"\"\"Preprocess steps for BERT: tokenize and pad sentences.\n",
    "    \n",
    "    Arguments:\n",
    "        text (pandas.Series): 1-D array of text to classify.\n",
    "        \n",
    "    Returns:\n",
    "        numpy.ndarray: A 2-D array of sentences, each sentence is\n",
    "            broken into an array of IDs for BERT.\n",
    "        numpy.ndarray: A 2-D array to mask padded IDs in a\n",
    "            sentence.\n",
    "    \"\"\"\n",
    "    \n",
    "    # tokenize\n",
    "    features = text.apply(\n",
    "        lambda x: bert_tokenizer.encode(x, add_special_tokens=True)\n",
    "    )\n",
    "\n",
    "    # pad sentences to make them the same length\n",
    "    max_len = 0\n",
    "    for s in features.values:\n",
    "        max_len = max(len(s), max_len)\n",
    "    features = np.array(\n",
    "        [s + [0] * (max_len - len(s)) for s in features.values]\n",
    "    )\n",
    "    \n",
    "    # mask\n",
    "    attention_mask = np.where(features != 0, 1, 0)\n",
    "    \n",
    "    return features, attention_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acknowledged-poultry",
   "metadata": {},
   "source": [
    "### BERT classification\n",
    "\n",
    "BERT can only classify sentences of up to 512 tokens (roughly 200 words). Longer sentences would have to be broken up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "boring-candidate",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bert_classify(text):\n",
    "    \"\"\"Use BERT to classify sentences. I think classes are\n",
    "        pretty much just numbers in a linear output space.\n",
    "        \n",
    "    Arguments:\n",
    "        text (pandas.Series): 1-D array of text to classify.\n",
    "            \n",
    "    Returns:\n",
    "        numpy.ndarray: Class of each sentence.\n",
    "    \"\"\"\n",
    "    \n",
    "    features, attention_mask = bert_preprocess(text)\n",
    "    features = torch.tensor(features)\n",
    "    attention_mask = torch.tensor(attention_mask)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        last_hidden_states = bert_model(features, attention_mask=attention_mask)\n",
    "\n",
    "    # return classes for each sentence\n",
    "    return last_hidden_states[0][:, 0,:].numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alien-digit",
   "metadata": {},
   "source": [
    "### this step takes a while"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "polish-citation",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_classes = bert_classify(df[\"text\"])\n",
    "print(bert_classes.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interim-thompson",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "streaming-conspiracy",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    bert_classes,\n",
    "    df[\"class\"],\n",
    "    test_size=0.2\n",
    ")\n",
    "\n",
    "svm_model = svm.SVC(kernel=\"linear\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "weekly-gather",
   "metadata": {},
   "source": [
    "### train SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "continuing-interview",
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_model.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adult-portland",
   "metadata": {},
   "source": [
    "### test SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "female-tucson",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = svm_model.predict(x_test)\n",
    "\n",
    "df_test = df[df.index.isin(y_test.index)]\n",
    "df_test = df_test.assign(pred=y_hat)\n",
    "\n",
    "report = classification_report(y_test, y_hat, output_dict=True)\n",
    "\n",
    "print(\"positive\\n\", report[\"1\"], end=\"\\n\\n\")\n",
    "print(\"negative\\n\", report[\"0\"], end=\"\\n\\n\")\n",
    "print(df_test.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "super-independence",
   "metadata": {},
   "source": [
    "### check some misclassifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "seven-lewis",
   "metadata": {},
   "outputs": [],
   "source": [
    "shown = 0\n",
    "print(\"1 is positive\", end=\"\\n\\n\")\n",
    "for _, row in df_test.iterrows():\n",
    "    p = row[\"pred\"]\n",
    "    a = row[\"class\"]\n",
    "    if p != a:\n",
    "        print(\"text:\", row[\"text\"])\n",
    "        print(f\"predicted {p}, actual {a}\", end=\"\\n\\n\")\n",
    "        shown += 1\n",
    "        if shown == 5:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "german-train",
   "metadata": {},
   "source": [
    "## play with robots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comic-footwear",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = input()\n",
    "while s != \"exit\":\n",
    "    input_bert_class = bert_classify(pd.Series(data=[s]))\n",
    "    input_pred = svm_model.predict(input_bert_class)\n",
    "    print(\"Predicted: \", \"positive\" if input_pred == 1 else \"negative\", end=\"\\n\\n\")\n",
    "    s = input()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "optimum-boards",
   "metadata": {},
   "source": [
    "## pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "particular-piece",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(svm_model, open(\"svm_model_2.sav\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faced-cooler",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sa-svm",
   "language": "python",
   "name": "sa-svm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
