{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "impossible-serve",
   "metadata": {},
   "source": [
    "# SVM 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "otherwise-tragedy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "saved-venue",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import logging\n",
    "import transformers as ppb\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "handmade-jurisdiction",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomState(MT19937) at 0x7FF70AFE16B0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.RandomState(123)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "capable-religious",
   "metadata": {},
   "source": [
    "## load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "entertaining-diameter",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   class                                               text\n",
      "0      0   As a woman you shouldn't complain about clean...\n",
      "1      0       momma said no pussy cats inside my doghouse \n",
      "2      0      SimplyAddictedToGuys woof woof hot scally lad\n",
      "3      0                            woof woof and hot soles\n",
      "4      0     Lemmie eat a Oreo do these dishes One oreo Lol\n",
      "(5593, 2)\n",
      "\n",
      " As a woman you shouldn't complain about cleaning up your house as a man you should always take the trash out \n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('../data/cleandata/twitter-c.csv')\n",
    "df = df.drop(labels=['Unnamed: 0'], axis=1)\n",
    "\n",
    "print(df.head())\n",
    "print(df.shape, end='\\n\\n')\n",
    "print(df.iat[0, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "casual-testimony",
   "metadata": {},
   "source": [
    "### reduce corpus size for convenience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "billion-rebel",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2500, 2)\n"
     ]
    }
   ],
   "source": [
    "df = df[:2500]\n",
    "\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hidden-entity",
   "metadata": {},
   "source": [
    "## load pretrained BERT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fewer-singles",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_tokenizer = ppb.DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "bert_model = ppb.DistilBertModel.from_pretrained('distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stone-clock",
   "metadata": {},
   "source": [
    "## use BERT\n",
    "\n",
    "### BERT preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "identical-fundamentals",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bert_preprocess(text):\n",
    "    '''Preprocess steps for BERT: tokenize and pad sentences.\n",
    "    \n",
    "    Arguments:\n",
    "        text (pandas.Series): 1-D array of text to classify.\n",
    "        \n",
    "    Returns:\n",
    "        numpy.ndarray: A 2-D array of sentences, each sentence is\n",
    "            broken into an array of IDs for BERT.\n",
    "        numpy.ndarray: A 2-D array to mask padded IDs in a\n",
    "            sentence.\n",
    "    '''\n",
    "    \n",
    "    # tokenize\n",
    "    features = text.apply(\n",
    "        lambda x: bert_tokenizer.encode(x, add_special_tokens=True)\n",
    "    )\n",
    "\n",
    "    # pad sentences to make them the same length\n",
    "    max_len = 0\n",
    "    for s in features.values:\n",
    "        max_len = max(len(s), max_len)\n",
    "    features = np.array(\n",
    "        [s + [0] * (max_len - len(s)) for s in features.values]\n",
    "    )\n",
    "    \n",
    "    # mask\n",
    "    attention_mask = np.where(features != 0, 1, 0)\n",
    "    \n",
    "    return features, attention_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acknowledged-poultry",
   "metadata": {},
   "source": [
    "### BERT classification\n",
    "\n",
    "BERT can only classify sentences of up to 512 tokens (roughly 200 words). Longer sentences would have to be broken up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "boring-candidate",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bert_classify(text):\n",
    "    '''Use BERT to classify sentences. I think classes are\n",
    "        pretty much just numbers in a linear output space.\n",
    "        \n",
    "    Arguments:\n",
    "        text (pandas.Series): 1-D array of text to classify.\n",
    "            \n",
    "    Returns:\n",
    "        pandas.DataFrame: Embeddings for each sentence.\n",
    "    '''\n",
    "    \n",
    "    features, attention_mask = bert_preprocess(text)\n",
    "    features = torch.tensor(features)\n",
    "    attention_mask = torch.tensor(attention_mask)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        last_hidden_states = bert_model(features, attention_mask=attention_mask)\n",
    "\n",
    "    # return classes for each sentence\n",
    "    return pd.DataFrame(data=last_hidden_states[0][:, 0,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alien-digit",
   "metadata": {},
   "source": [
    "### this step takes a while"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "polish-citation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2500, 768)\n"
     ]
    }
   ],
   "source": [
    "bert_classes = bert_classify(df['text'])\n",
    "print(bert_classes.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interim-thompson",
   "metadata": {},
   "source": [
    "## Split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "streaming-conspiracy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000, 768) (2000,)\n",
      "(250, 768) (250,)\n",
      "(250, 768) (250,)\n"
     ]
    }
   ],
   "source": [
    "x_, x_test, y_, y_test = train_test_split(\n",
    "    bert_classes,\n",
    "    df['class'],\n",
    "    test_size=0.1,\n",
    "    stratify=df['class']\n",
    ")\n",
    "\n",
    "x_train, x_val, y_train, y_val = train_test_split(\n",
    "    x_,\n",
    "    y_,\n",
    "    test_size=1/9,\n",
    "    stratify=y_\n",
    ")\n",
    "\n",
    "print(x_train.shape, y_train.shape)\n",
    "print(x_val.shape, y_val.shape)\n",
    "print(x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "weekly-gather",
   "metadata": {},
   "source": [
    "## SVM\n",
    "\n",
    "todo\n",
    "\n",
    "1. normalize data\n",
    "2. improve hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "composed-samoa",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = svm.SVC(kernel='linear')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "empty-block",
   "metadata": {},
   "source": [
    "### train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "continuing-interview",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.937"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train)\n",
    "model.score(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "weird-hammer",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assess_model(model, x, y):\n",
    "    '''Predict labels with a model and see the classification report.\n",
    "    \n",
    "    Arguments:\n",
    "        model: a model.\n",
    "        x (pandas.DataFrame): Embeddings for each sentence.\n",
    "        y (pandas.Series): Classes for each sentence.\n",
    "        \n",
    "    Returns:\n",
    "        pandas.DataFrame: Sentences, predictions, and their true classes.\n",
    "    '''\n",
    "    \n",
    "    y_pred = model.predict(x)\n",
    "\n",
    "    df_pred = df[df.index.isin(y.index)]\n",
    "    df_pred = df_pred.assign(pred=y_pred)\n",
    "\n",
    "    report = classification_report(y, y_pred, output_dict=True)\n",
    "\n",
    "    print('safe\\n', report['0'], end='\\n\\n')\n",
    "    print('hate\\n', report['1'], end='\\n\\n')\n",
    "\n",
    "    return df_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adult-portland",
   "metadata": {},
   "source": [
    "### validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "female-tucson",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "safe\n",
      " {'precision': 0.875, 'recall': 0.8953488372093024, 'f1-score': 0.8850574712643678, 'support': 172}\n",
      "\n",
      "hate\n",
      " {'precision': 0.7567567567567568, 'recall': 0.717948717948718, 'f1-score': 0.736842105263158, 'support': 78}\n",
      "\n",
      "    class                                               text  pred\n",
      "11      0   I'm an early bird and I'm a night owl so I'm ...     1\n",
      "15      0   this the I play soccer cheat on girls and wea...     0\n",
      "27      0   10 birds your grandkids may never see thanks ...     0\n",
      "64      0   Fit lads Nice gear these scally lads n traine...     1\n",
      "67      0   RAWR My sexy French scally I love him frenchs...     0\n"
     ]
    }
   ],
   "source": [
    "df_val = assess_model(model, x_val, y_val)\n",
    "print(df_val.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "super-independence",
   "metadata": {},
   "source": [
    "### check some misclassifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "seven-lewis",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 is hate\n",
      "\n",
      "text:  I'm an early bird and I'm a night owl so I'm wise and have worms \n",
      "predicted 1, actual 0\n",
      "\n",
      "text:  Fit lads Nice gear these scally lads n trainers would get it\n",
      "predicted 1, actual 0\n",
      "\n",
      "text:  hick and raver is a venn diagram that has a very large intersection \n",
      "predicted 1, actual 0\n",
      "\n",
      "text:  California is full of white trash who moved from Oklahoma\n",
      "predicted 0, actual 1\n",
      "\n",
      "text:  FireCashman Why Because I am having to root for the Royals in October Yankees\n",
      "predicted 1, actual 0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "shown = 0\n",
    "print('1 is hate', end='\\n\\n')\n",
    "for _, row in df_val.iterrows():\n",
    "    p = row['pred']\n",
    "    a = row['class']\n",
    "    if p != a:\n",
    "        print('text:', row['text'])\n",
    "        print(f'predicted {p}, actual {a}', end='\\n\\n')\n",
    "        shown += 1\n",
    "        if shown == 5:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "african-nirvana",
   "metadata": {},
   "source": [
    "## try more models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cognitive-mandate",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/charlotte/anaconda3/envs/sa-svm/lib/python3.7/site-packages/sklearn/svm/_base.py:258: ConvergenceWarning: Solver terminated early (max_iter=1200).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy: 0.9445\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.856"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_lin = svm.SVC(kernel='linear', max_iter=1200)\n",
    "model_lin.fit(x_train, y_train)\n",
    "print('train accuracy:', model_lin.score(x_train, y_train))\n",
    "model_lin.score(x_val, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "german-train",
   "metadata": {},
   "source": [
    "## play with robots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "comic-footwear",
   "metadata": {},
   "outputs": [],
   "source": [
    "def use_model(model):\n",
    "    s = input()\n",
    "    while s != 'exit':\n",
    "        input_bert_class = bert_classify(pd.Series(data=[s]))\n",
    "        input_pred = model.predict(input_bert_class)\n",
    "        print('Predicted:', 'safe' if input_pred == 0 else 'hate', end='\\n\\n')\n",
    "        s = input()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "excellent-keeping",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "if your best pitch for your revolutionary new technology is please don’t criticize it maybe it just sucks\n",
      "Predicted: safe\n",
      "\n",
      "it’s not clever, it’s not actually that interesting, and it’s only original because no one was stupid enough to do it before. it’s not some special application of anything, it’s just a demonstration of what happens when you throw impractical, industrial levels of energy at the tiniest problem\n",
      "Predicted: safe\n",
      "\n",
      "exit\n"
     ]
    }
   ],
   "source": [
    "use_model(model_lin)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "optimum-boards",
   "metadata": {},
   "source": [
    "## pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "particular-piece",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(model, open('svm_model_3.sav', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "according-lincoln",
   "metadata": {},
   "source": [
    "## test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comic-honor",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = assess_model(model, x_test, y_test)\n",
    "print(df_test.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faced-cooler",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sa-svm",
   "language": "python",
   "name": "sa-svm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
